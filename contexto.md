# Contexto do Projeto: Sistema de CorreÃ§Ã£o AutomÃ¡tica de Assignments

## ğŸ“‹ VisÃ£o Geral

Este documento registra o contexto, decisÃµes de design e padrÃµes estabelecidos durante o desenvolvimento do **Sistema de CorreÃ§Ã£o AutomÃ¡tica de Assignments**. O projeto Ã© uma ferramenta educacional que combina anÃ¡lise de IA (OpenAI GPT) com execuÃ§Ã£o de testes automatizados para avaliar trabalhos de programaÃ§Ã£o.

## ğŸ—ï¸ Arquitetura e DecisÃµes de Design

### Estrutura de Camadas (Clean Architecture)

O projeto segue uma arquitetura em camadas bem definida:

```
src/
â”œâ”€â”€ domain/           # Camada de domÃ­nio (entidades e regras de negÃ³cio)
â”œâ”€â”€ repositories/     # Camada de acesso a dados
â”œâ”€â”€ services/         # Camada de serviÃ§os (lÃ³gica de aplicaÃ§Ã£o)
â”œâ”€â”€ utils/            # UtilitÃ¡rios e helpers
â””â”€â”€ main.py          # Ponto de entrada (CLI)
```

**DecisÃ£o**: SeparaÃ§Ã£o clara de responsabilidades usando Clean Architecture para facilitar manutenÃ§Ã£o e testes.

### PadrÃµes de Naming

#### Classes e MÃ³dulos
- **Classes de ServiÃ§o**: Sufixo `Service` (ex: `CorrectionService`, `AIAnalyzer`)
- **Classes de RepositÃ³rio**: Sufixo `Repository` (ex: `AssignmentRepository`, `SubmissionRepository`)
- **Classes de UtilitÃ¡rio**: Sufixo `Generator` ou `Manager` (ex: `ReportGenerator`, `PromptManager`)
- **Classes de Teste**: Sufixo `Test` (ex: `PytestExecutor`)

#### VariÃ¡veis e MÃ©todos
- **VariÃ¡veis**: snake_case (ex: `assignment_name`, `submission_path`)
- **MÃ©todos**: snake_case (ex: `correct_assignment`, `calculate_final_score`)
- **Constantes**: UPPER_SNAKE_CASE (ex: `OPENAI_API_KEY`, `TEST_TIMEOUT`)
- **Enums**: PascalCase (ex: `AssignmentType`, `SubmissionType`)

#### Arquivos e DiretÃ³rios
- **MÃ³dulos Python**: snake_case (ex: `correction_service.py`, `ai_analyzer.py`)
- **DiretÃ³rios**: snake_case (ex: `enunciados/`, `respostas/`, `reports/`)
- **ConfiguraÃ§Ãµes**: `config.py` (centralizado)

## ğŸ¯ DecisÃµes de Design Principais

### 1. Suporte a SubmissÃµes Individuais e em Grupo

**DecisÃ£o**: Sistema flexÃ­vel que suporta tanto submissÃµes individuais quanto em grupo, configurado por assignment.

**ImplementaÃ§Ã£o**:
```python
# config.py
ASSIGNMENT_SUBMISSION_TYPES = {
    "prog1-tarefa-html-curriculo": SubmissionType.INDIVIDUAL,
    "prog1-prova-av": SubmissionType.GROUP,
}
```

**PadrÃ£o**: ConfiguraÃ§Ã£o centralizada em `config.py` para facilitar manutenÃ§Ã£o.

### 2. Sistema de Prompts Personalizados

**DecisÃ£o**: Prompts especÃ­ficos por assignment para anÃ¡lise mais precisa da IA.

**Estrutura**:
```
prompts/
â”œâ”€â”€ assignment-name/
â”‚   â””â”€â”€ prompt.txt    # Prompt personalizado
```

**PadrÃ£o**: Prompts versionados separadamente dos enunciados para controle de versÃ£o.

### 3. Sistema de Logs de Auditoria

**DecisÃ£o**: Logs completos de todas as anÃ¡lises da IA para transparÃªncia e auditoria.

**Estrutura**:
```
logs/
â”œâ”€â”€ YYYY-MM-DD/
â”‚   â””â”€â”€ assignment-name/
â”‚       â””â”€â”€ submission_analysis_timestamp.json
```

**PadrÃ£o**: Logs nÃ£o versionados (`.gitignore`) mas estruturados para fÃ¡cil consulta.

### 4. MÃºltiplos Formatos de RelatÃ³rio

**DecisÃ£o**: Suporte a mÃºltiplos formatos de saÃ­da (console, HTML, Markdown, JSON).

**PadrÃ£o**: 
- JSON como formato base (dados estruturados)
- ConversÃ£o para outros formatos sem re-execuÃ§Ã£o
- Comandos separados para conversÃ£o (`convert-report`, `convert-latest`)

## ğŸ“ PadrÃµes de DocumentaÃ§Ã£o e ComentÃ¡rios

### Docstrings

**PadrÃ£o**: Docstrings simples e concisas, preferencialmente apenas com descriÃ§Ã£o. Campos adicionais (Args, Returns, Raises) apenas quando necessÃ¡rio para esclarecimento.

**Exemplo de docstring simples**:
```python
def correct_assignment(self, assignment_name: str, turma_name: str, 
                      submission_identifier: Optional[str] = None) -> CorrectionReport:
    """Corrige um assignment especÃ­fico."""
```

**Exemplo de docstring com campos adicionais** (apenas quando necessÃ¡rio):
```python
def _save_ai_log(self, assignment_name: str, submission_identifier: str, 
                 analysis_type: str, prompt: str, raw_response: str, 
                 parsed_result: dict) -> None:
    """Salva log de auditoria da anÃ¡lise de IA.
    
    Args:
        assignment_name: Nome do assignment sendo analisado
        submission_identifier: Identificador da submissÃ£o (login ou grupo)
        analysis_type: Tipo de anÃ¡lise (python, html)
        prompt: Prompt enviado para a IA
        raw_response: Resposta raw da IA
        parsed_result: Resultado processado da anÃ¡lise
    """
```

**PrincÃ­pio**: Manter documentaÃ§Ã£o simples e adicionar detalhes apenas quando o mÃ©todo/parÃ¢metro nÃ£o for autoexplicativo.

### ComentÃ¡rios Inline

**PadrÃ£o**: ComentÃ¡rios explicativos para lÃ³gica complexa, nÃ£o para cÃ³digo Ã³bvio.

```python
# Calcula nota final ponderada
final_score = (test_score * 0.4) + (ai_score * 0.6)
```

### README.md

**PadrÃ£o**: DocumentaÃ§Ã£o completa com exemplos prÃ¡ticos, incluindo:
- InstalaÃ§Ã£o e configuraÃ§Ã£o
- Exemplos de uso para cada comando
- ExplicaÃ§Ã£o da estrutura do projeto
- Troubleshooting comum

## ğŸ”§ PadrÃµes de ConfiguraÃ§Ã£o

### ConfiguraÃ§Ã£o Centralizada

**DecisÃ£o**: Todas as configuraÃ§Ãµes em `config.py` para facilitar manutenÃ§Ã£o.

```python
# ConfiguraÃ§Ãµes da API OpenAI
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = "gpt-3.5-turbo"
OPENAI_MAX_TOKENS = 1000
OPENAI_TEMPERATURE = 0.3

# ConfiguraÃ§Ãµes de teste
TEST_TIMEOUT = 30  # segundos
MAX_TEST_OUTPUT = 1000  # caracteres
```

### ConfiguraÃ§Ã£o de API Keys

**PadrÃ£o**: Busca automÃ¡tica da API key em ordem de prioridade:
1. VariÃ¡vel de ambiente `OPENAI_API_KEY`
2. Arquivo `~/.secrets/open-ai-api-key.txt`
3. Arquivo `.secrets/open-ai-api-key.txt`

## ğŸ§ª PadrÃµes de Teste

### Estrutura de Testes

**PadrÃ£o**: Testes organizados por mÃ³dulo com prefixo `test_`.

```
tests/
â”œâ”€â”€ test_models.py      # Testes dos modelos de domÃ­nio
â”œâ”€â”€ test_services.py    # Testes dos serviÃ§os
â””â”€â”€ logs/              # Logs de teste (nÃ£o versionados)
```

### Naming de Testes

**PadrÃ£o**: `test_{method_name}_{scenario}`

```python
def test_correct_assignment_single_submission(self):
    """Testa correÃ§Ã£o de assignment com submissÃ£o Ãºnica."""

def test_correct_assignment_all_submissions(self):
    """Testa correÃ§Ã£o de assignment com todas as submissÃµes."""
```

## ğŸ¨ PadrÃµes de UI/UX (CLI)

### Uso do Rich para Interface

**DecisÃ£o**: Biblioteca Rich para interface CLI rica e colorida.

**PadrÃ£o**:
- PainÃ©is para tÃ­tulos de seÃ§Ãµes
- Cores consistentes (azul para tÃ­tulos, verde para sucesso, vermelho para erro)
- Barras de progresso para operaÃ§Ãµes longas

```python
console.print(Panel(f"[bold blue]Corrigindo assignment {assignment} da turma {turma}[/bold blue]"))

with Progress(SpinnerColumn(), TextColumn("[progress.description]{task.description}")) as progress:
    task = progress.add_task("Processando submissÃµes...", total=None)
```

### Comandos CLI

**PadrÃ£o**: Comandos verbosos e descritivos com opÃ§Ãµes bem documentadas.

```python
@cli.command()
@click.option('--assignment', '-a', help='Nome do assignment para corrigir')
@click.option('--turma', '-t', required=True, help='Nome da turma')
@click.option('--submissao', '-s', help='Identificador da submissÃ£o')
```

## ğŸ”„ PadrÃµes de Processamento

### Tratamento de Erros

**PadrÃ£o**: ExceÃ§Ãµes especÃ­ficas com mensagens claras e logging apropriado.

```python
try:
    report = correction_service.correct_assignment(assignment, turma, submissao)
except ValueError as e:
    console.print(f"[red]Erro: {str(e)}[/red]")
    sys.exit(1)
except Exception as e:
    console.print(f"[red]Erro inesperado: {str(e)}[/red]")
    sys.exit(1)
```

### Processamento AssÃ­ncrono

**DecisÃ£o**: Processamento sequencial com feedback visual para operaÃ§Ãµes longas.

**PadrÃ£o**: Uso de Progress bars e spinners para manter o usuÃ¡rio informado.

## ğŸ“Š PadrÃµes de Dados

### Modelos de DomÃ­nio

**PadrÃ£o**: Dataclasses para representar entidades do domÃ­nio.

```python
@dataclass
class IndividualSubmission:
    """SubmissÃ£o individual de um aluno."""
    github_login: str
    assignment_name: str
    turma: str
    submission_path: Path
    files: List[str] = field(default_factory=list)
    test_results: List[AssignmentTestExecution] = field(default_factory=list)
    code_analysis: Optional[CodeAnalysis] = None
    html_analysis: Optional[HTMLAnalysis] = None
    final_score: float = 0.0
    feedback: str = ""
```

### SerializaÃ§Ã£o JSON

**PadrÃ£o**: MÃ©todos `to_dict()` e `load_from_file()` para persistÃªncia.

```python
def to_dict(self) -> Dict[str, Any]:
    """Converte o relatÃ³rio para dicionÃ¡rio."""
    return {
        "assignment_name": self.assignment_name,
        "turma": self.turma,
        "generated_at": self.generated_at,
        # ...
    }
```

## ğŸš€ PadrÃµes de Deploy e DistribuiÃ§Ã£o

### Estrutura de DependÃªncias

**DecisÃ£o**: Pipenv para gerenciamento de dependÃªncias e ambiente virtual.

**PadrÃ£o**: 
- `Pipfile` para dependÃªncias principais
- `Pipfile.lock` para versÃµes fixas
- `setup.py` para instalaÃ§Ã£o como pacote

### Estrutura de DiretÃ³rios

**PadrÃ£o**: SeparaÃ§Ã£o clara entre cÃ³digo versionado e dados nÃ£o versionados.

```
corrige-assignments/
â”œâ”€â”€ src/              # CÃ³digo fonte (versionado)
â”œâ”€â”€ enunciados/       # Enunciados (nÃ£o versionado)
â”œâ”€â”€ respostas/        # SubmissÃµes (nÃ£o versionado)
â”œâ”€â”€ reports/          # RelatÃ³rios (nÃ£o versionado)
â”œâ”€â”€ logs/             # Logs (nÃ£o versionado)
â””â”€â”€ prompts/          # Prompts (versionado)
```

## ğŸ” PadrÃµes de Debugging e Monitoramento

### Logging Estruturado

**PadrÃ£o**: Logs JSON estruturados com metadados completos.

```json
{
  "metadata": {
    "assignment_name": "prog1-prova-av",
    "submission_identifier": "joao-silva",
    "analysis_type": "python",
    "timestamp": "2025-01-15T10:30:14.095265",
    "ai_model": "gpt-3.5-turbo"
  },
  "prompt": "...",
  "raw_response": "...",
  "parsed_result": {...}
}
```

### Tratamento de Warnings

**DecisÃ£o**: EliminaÃ§Ã£o de warnings do pytest para manter cÃ³digo limpo.

**PadrÃ£o**: RenomeaÃ§Ã£o de classes que conflitam com pytest (ex: `TestResult` â†’ `AssignmentTestResult`).

## ğŸ“ˆ PadrÃµes de Performance

### ExecuÃ§Ã£o de Testes

**DecisÃ£o**: ExecuÃ§Ã£o direta na pasta do aluno sem cÃ³pia para performance.

**PadrÃ£o**: Timeout configurÃ¡vel e limitaÃ§Ã£o de output para evitar travamentos.

```python
TEST_TIMEOUT = 30  # segundos
MAX_TEST_OUTPUT = 1000  # caracteres
```

### AnÃ¡lise de IA

**DecisÃ£o**: ConfiguraÃ§Ãµes otimizadas para balancear qualidade e custo.

**PadrÃ£o**: 
- Modelo: `gpt-3.5-turbo` (bom custo-benefÃ­cio)
- Max tokens: 1000 (suficiente para anÃ¡lise)
- Temperature: 0.3 (consistente mas nÃ£o muito rÃ­gido)

## ğŸ¯ Diretrizes para Futuras ImplementaÃ§Ãµes

### Novos Recursos

1. **Mantenha a arquitetura em camadas**
2. **Use os padrÃµes de naming estabelecidos**
3. **Documente com docstrings no estilo Google**
4. **Configure novos assignments em `config.py`**
5. **Crie prompts personalizados quando necessÃ¡rio**
6. **Implemente logs de auditoria para operaÃ§Ãµes crÃ­ticas**
7. **Mantenha compatibilidade com formatos de relatÃ³rio existentes**

### RefatoraÃ§Ãµes

1. **Preserve a interface pÃºblica dos serviÃ§os**
2. **Mantenha compatibilidade com relatÃ³rios JSON existentes**
3. **Atualize documentaÃ§Ã£o quando necessÃ¡rio**
4. **Execute testes apÃ³s mudanÃ§as**
5. **Verifique se nÃ£o hÃ¡ warnings do pytest**

### ManutenÃ§Ã£o

1. **Revise logs de auditoria periodicamente**
2. **Atualize prompts baseado no feedback**
3. **Monitore performance de testes**
4. **Mantenha dependÃªncias atualizadas**
5. **Verifique configuraÃ§Ãµes de API keys**

---

**Ãšltima atualizaÃ§Ã£o**: Janeiro 2025  
**VersÃ£o do projeto**: 1.0.0  
**Mantenedor**: Jefferson Santos (jefferson.santos@fgv.br) 