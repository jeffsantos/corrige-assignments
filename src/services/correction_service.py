"""
ServiÃ§o principal de correÃ§Ã£o que orquestra todo o processo.
"""
from datetime import datetime
from pathlib import Path
from typing import List, Optional
from ..domain.models import (
    IndividualSubmission, GroupSubmission, Submission, Assignment, CorrectionReport, 
    AssignmentType, AssignmentTestResult
)
from ..repositories.assignment_repository import AssignmentRepository
from ..repositories.submission_repository import SubmissionRepository
from .test_executor import PytestExecutor
from .ai_analyzer import AIAnalyzer
from .streamlit_thumbnail_service import StreamlitThumbnailService
from .html_thumbnail_service import HTMLThumbnailService


class CorrectionService:
    """ServiÃ§o principal de correÃ§Ã£o."""
    
    def __init__(self, enunciados_path: Path, respostas_path: Path, openai_api_key: str = None, logs_path: Path = None, verbose: bool = False):
        self.assignment_repo = AssignmentRepository(enunciados_path)
        self.submission_repo = SubmissionRepository(respostas_path)
        self.test_executor = PytestExecutor()
        self.ai_analyzer = AIAnalyzer(openai_api_key, enunciados_path, logs_path)
        self.streamlit_thumbnail_service = StreamlitThumbnailService(verbose=verbose)
        self.html_thumbnail_service = HTMLThumbnailService(verbose=verbose)
    
    def correct_assignment(self, assignment_name: str, turma_name: str, 
                          submission_identifier: Optional[str] = None) -> CorrectionReport:
        """Corrige um assignment especÃ­fico."""
        # Carrega o assignment
        assignment = self.assignment_repo.get_assignment(assignment_name)
        if not assignment:
            raise ValueError(f"Assignment '{assignment_name}' nÃ£o encontrado")
        
        # Carrega as submissÃµes
        if submission_identifier:
            submissions = [self.submission_repo.get_submission(turma_name, assignment_name, submission_identifier)]
            submissions = [s for s in submissions if s is not None]
        else:
            submissions = self.submission_repo.get_submissions_for_assignment(turma_name, assignment_name)
        
        if not submissions:
            raise ValueError(f"Nenhuma submissÃ£o encontrada para {assignment_name} na turma {turma_name}")
        
        # Processa cada submissÃ£o
        for submission in submissions:
            try:
                self._process_submission(submission, assignment)
            except Exception as e:
                print(f"âŒ Erro ao processar submissÃ£o {submission.display_name}: {e}")
                # Continua com a prÃ³xima submissÃ£o
                continue
        
        # Cria o relatÃ³rio
        report = CorrectionReport(
            assignment_name=assignment_name,
            turma=turma_name,
            submissions=submissions,
            generated_at=datetime.now().isoformat()
        )
        
        # Calcula estatÃ­sticas do relatÃ³rio
        report.summary = self._calculate_summary(submissions)
        
        # Gera thumbnails se o assignment suportar
        from config import assignment_has_thumbnails, get_assignment_thumbnail_type
        
        if assignment_has_thumbnails(assignment_name):
            thumbnail_type = get_assignment_thumbnail_type(assignment_name)
            print(f"\nðŸ–¼ï¸  Iniciando geraÃ§Ã£o de thumbnails {thumbnail_type.upper()} para {len(submissions)} submissÃµes...")
            try:
                if thumbnail_type == "streamlit":
                    report.thumbnails = self.streamlit_thumbnail_service.generate_thumbnails_for_assignment(
                        assignment_name, turma_name, submissions
                    )
                elif thumbnail_type == "html":
                    report.thumbnails = self.html_thumbnail_service.generate_thumbnails_for_assignment(
                        assignment_name, turma_name, submissions
                    )
                print(f"âœ… GeraÃ§Ã£o de thumbnails {thumbnail_type.upper()} concluÃ­da: {len(report.thumbnails)} thumbnails gerados")
            except Exception as e:
                print(f"âŒ Erro ao gerar thumbnails {thumbnail_type.upper()}: {e}")
                # Inicializa lista vazia para evitar erro
                report.thumbnails = []
        
        return report
    
    def correct_all_assignments(self, turma_name: str) -> List[CorrectionReport]:
        """Corrige todos os assignments de uma turma."""
        turma = self.submission_repo.get_turma(turma_name)
        if not turma:
            raise ValueError(f"Turma '{turma_name}' nÃ£o encontrada")
        
        reports = []
        
        for assignment_name in turma.assignments:
            try:
                report = self.correct_assignment(assignment_name, turma_name)
                reports.append(report)
            except Exception as e:
                print(f"Erro ao corrigir {assignment_name}: {e}")
                continue
        
        return reports
    
    def _process_submission(self, submission: Submission, assignment: Assignment):
        """Processa uma submissÃ£o."""
        print(f"Processando submissÃ£o de {submission.display_name}...")
        
        try:
            # Executa testes se for assignment Python
            if assignment.type == AssignmentType.PYTHON and assignment.test_files:
                submission.test_results = self.test_executor.run_tests(
                    submission.submission_path, 
                    assignment.test_files
                )
        except Exception as e:
            print(f"  âš ï¸  Erro nos testes para {submission.display_name}: {e}")
            submission.test_results = []
        
        try:
            # Analisa cÃ³digo usando IA
            if assignment.type == AssignmentType.PYTHON:
                submission.code_analysis = self.ai_analyzer.analyze_python_code(
                    submission.submission_path, 
                    assignment
                )
            else:  # HTML
                submission.html_analysis = self.ai_analyzer.analyze_html_code(
                    submission.submission_path, 
                    assignment
                )
        except Exception as e:
            print(f"  âš ï¸  Erro na anÃ¡lise de IA para {submission.display_name}: {e}")
            if assignment.type == AssignmentType.PYTHON:
                submission.code_analysis = None
            else:
                submission.html_analysis = None
        
        # Calcula nota final
        submission.final_score = self._calculate_final_score(submission, assignment)
        
        # Gera feedback
        submission.feedback = self._generate_feedback(submission, assignment)
    
    def _calculate_final_score(self, submission: Submission, assignment: Assignment) -> float:
        """Calcula a nota final baseada nos critÃ©rios da rubrica."""
        if assignment.type == AssignmentType.PYTHON:
            return self._calculate_python_score(submission, assignment)
        else:
            return self._calculate_html_score(submission, assignment)
    
    def _calculate_python_score(self, submission: Submission, assignment: Assignment) -> float:
        """Calcula nota para assignments Python."""
        rubric = assignment.rubric
        
        # Nota dos testes (40% do peso total)
        test_score = 0.0
        if submission.test_results:
            passed_tests = sum(1 for test in submission.test_results if test.result == AssignmentTestResult.PASSED)
            total_tests = len(submission.test_results)
            if total_tests > 0:
                test_score = (passed_tests / total_tests) * 10.0
        
        # Nota da anÃ¡lise de IA (60% do peso total)
        ai_score = 0.0
        if submission.code_analysis:
            ai_score = submission.code_analysis.score
        
        # Calcula nota final ponderada
        final_score = (test_score * 0.4) + (ai_score * 0.6)
        
        return min(10.0, max(0.0, final_score))
    
    def _calculate_html_score(self, submission: Submission, assignment: Assignment) -> float:
        """Calcula nota para assignments HTML."""
        if submission.html_analysis:
            return submission.html_analysis.score
        return 0.0
    
    def _generate_feedback(self, submission: Submission, assignment: Assignment) -> str:
        """Gera feedback personalizado para o aluno."""
        feedback_parts = []
        
        # Feedback dos testes
        if submission.test_results:
            passed_tests = sum(1 for test in submission.test_results if test.result == AssignmentTestResult.PASSED)
            total_tests = len(submission.test_results)
            feedback_parts.append(f"Testes: {passed_tests}/{total_tests} passaram")
            
            # Adiciona detalhes dos testes que falharam
            failed_tests = [test for test in submission.test_results if test.result == AssignmentTestResult.FAILED]
            if failed_tests:
                feedback_parts.append("Testes que falharam:")
                for test in failed_tests[:3]:  # Limita a 3 testes para nÃ£o ficar muito longo
                    feedback_parts.append(f"- {test.test_name}: {test.message[:100]}...")
        
        # Feedback da anÃ¡lise de IA
        if assignment.type == AssignmentType.PYTHON and submission.code_analysis:
            feedback_parts.append(f"AnÃ¡lise de cÃ³digo: {submission.code_analysis.score:.1f}/10")
            if submission.code_analysis.comments:
                feedback_parts.append("Pontos positivos:")
                for comment in submission.code_analysis.comments[:2]:
                    feedback_parts.append(f"- {comment}")
            if submission.code_analysis.issues_found:
                feedback_parts.append("Problemas encontrados:")
                for issue in submission.code_analysis.issues_found[:2]:
                    feedback_parts.append(f"- {issue}")
        
        elif assignment.type == AssignmentType.HTML and submission.html_analysis:
            feedback_parts.append(f"AnÃ¡lise HTML/CSS: {submission.html_analysis.score:.1f}/10")
            if submission.html_analysis.required_elements:
                feedback_parts.append("Elementos HTML:")
                for element, found in submission.html_analysis.required_elements.items():
                    status = "âœ“" if found else "âœ—"
                    feedback_parts.append(f"- {element}: {status}")
        
        return "\n".join(feedback_parts)
    
    def _calculate_summary(self, submissions: List[Submission]) -> dict:
        """Calcula estatÃ­sticas resumidas das submissÃµes."""
        if not submissions:
            return {}
        
        scores = [sub.final_score for sub in submissions]
        
        # Arredonda para uma casa decimal para consistÃªncia visual
        avg = round(sum(scores) / len(scores), 1)
        min_score = round(min(scores), 1)
        max_score = round(max(scores), 1)
        
        return {
            "total_submissions": len(submissions),
            "average_score": avg,
            "min_score": min_score,
            "max_score": max_score,
            "passing_rate": sum(1 for score in scores if score >= 6.0) / len(scores),
            "excellent_rate": sum(1 for score in scores if score >= 9.0) / len(scores)
        } 